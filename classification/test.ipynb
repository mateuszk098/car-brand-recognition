{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from classification.network.resnet import SeResNet\n",
    "from classification.utils.loaders import VehicleDataLoader, read_image\n",
    "from classification.utils.transforms import VehicleTransform\n",
    "\n",
    "train_dataset = ImageFolder(\"../data/vehicles/train\", loader=read_image)\n",
    "valid_dataset = ImageFolder(\"../data/vehicles/valid\", loader=read_image)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "transform = VehicleTransform(size=(224, 224))\n",
    "\n",
    "train_loader = VehicleDataLoader(\n",
    "    train_dataset,\n",
    "    train_transform=transform.train_transform,\n",
    "    eval_transform=transform.eval_transform,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_loader = VehicleDataLoader(\n",
    "    valid_dataset,\n",
    "    eval_transform=transform.eval_transform,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "train_loader.train()\n",
    "model = SeResNet(num_classes=6)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
       "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=None)\n",
       "    RandomGrayscale(p=0.2)\n",
       "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0467,  0.0160,  0.0514,  0.0083,  0.0402,  0.0492],\n",
       "        [ 0.0467,  0.0160,  0.0514,  0.0083,  0.0402,  0.0492],\n",
       "        [-0.9165,  0.9254, -0.0039, -0.5991,  0.5991, -0.7124],\n",
       "        [-1.4088, -0.8216,  1.7251,  0.6593,  0.6106, -0.0317],\n",
       "        [ 0.5141, -0.3288,  1.9802, -0.3713,  0.6967, -1.9077],\n",
       "        [ 0.3654,  0.0222, -0.0081, -0.0517,  0.2052,  0.1017],\n",
       "        [ 0.3654,  0.0222, -0.0081, -0.0517,  0.2052,  0.1017],\n",
       "        [ 0.3654,  0.0222, -0.0081, -0.0517,  0.2052,  0.1017]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1686, 0.1635, 0.1694, 0.1622, 0.1675, 0.1690],\n",
       "        [0.1686, 0.1635, 0.1694, 0.1622, 0.1675, 0.1690],\n",
       "        [0.0590, 0.3721, 0.1469, 0.0810, 0.2685, 0.0724],\n",
       "        [0.0221, 0.0398, 0.5084, 0.1751, 0.1668, 0.0877],\n",
       "        [0.1340, 0.0577, 0.5804, 0.0553, 0.1608, 0.0119],\n",
       "        [0.2138, 0.1517, 0.1472, 0.1409, 0.1822, 0.1642],\n",
       "        [0.2138, 0.1517, 0.1472, 0.1409, 0.1822, 0.1642],\n",
       "        [0.2138, 0.1517, 0.1472, 0.1409, 0.1822, 0.1642]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.softmax(pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x1f696978650>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "metric = MulticlassAccuracy(average=None, num_classes=6)\n",
    "metric.update(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1429, 0.3333, 0.6667, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from classification.network import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 244)\n",
    "imgs = torch.randn(10, 3, *img_size)\n",
    "\n",
    "img_width, img_height\n",
    "\n",
    "\n",
    "\n",
    "feed_forward = nn.Sequential(\n",
    "    #\n",
    "    nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "    nn.BatchNorm2d(num_features=32),\n",
    "    nn.Mish(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #\n",
    "    layers.SEResidualBlock(32, 64, kernel_size=3, stride=1, squeeze_active=True),\n",
    "    layers.SEResidualBlock(64, 64, kernel_size=3, stride=1, squeeze_active=True),\n",
    "    layers.MaxDepthPool2d(pool_size=2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #\n",
    "    layers.SEResidualBlock(32, 96, kernel_size=5, stride=1, squeeze_active=True),\n",
    "    layers.SEResidualBlock(96, 96, kernel_size=5, stride=1, squeeze_active=True),\n",
    "    layers.MaxDepthPool2d(pool_size=2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #\n",
    "    layers.SEResidualBlock(48, 128, kernel_size=3, stride=1, squeeze_active=True),\n",
    "    layers.SEResidualBlock(128, 128, kernel_size=3, stride=1, squeeze_active=True),\n",
    "    layers.MaxDepthPool2d(pool_size=4),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    #\n",
    "    nn.Flatten(),\n",
    "    #\n",
    "    nn.Linear(32 * 7 * 7, 256, bias=False),\n",
    "    nn.BatchNorm1d(num_features=256),\n",
    "    nn.Mish(),\n",
    "    nn.Dropout1d(0.4),\n",
    "    #\n",
    "    nn.Linear(256, 256, bias=False),\n",
    "    nn.BatchNorm1d(num_features=256),\n",
    "    nn.Mish(),\n",
    "    nn.Dropout1d(0.4),\n",
    "    #\n",
    "    nn.Linear(256, num_classes),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle-recognition-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
